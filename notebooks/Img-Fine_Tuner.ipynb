{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.onnx.symbolic_opset11 import hstack\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': './dataset/Train/Image/3 Summer Sangria Recipes-screenshot (2).jpg', 'annotation': './dataset/Train/Instance/3 Summer Sangria Recipes-screenshot (2).png'}, {'image': './dataset/Train/Image/6 Classic Cocktail Recipes!-screenshot (1).jpg', 'annotation': './dataset/Train/Instance/6 Classic Cocktail Recipes!-screenshot (1).png'}, {'image': './dataset/Train/Image/6 Classic Cocktail Recipes!-screenshot (2).jpg', 'annotation': './dataset/Train/Instance/6 Classic Cocktail Recipes!-screenshot (2).png'}, {'image': './dataset/Train/Image/6 Classic Cocktail Recipes!-screenshot.jpg', 'annotation': './dataset/Train/Instance/6 Classic Cocktail Recipes!-screenshot.png'}, {'image': './dataset/Train/Image/A Gas Phase Reaction- Producing Ammonium Chloride-screenshot (2).jpg', 'annotation': './dataset/Train/Instance/A Gas Phase Reaction- Producing Ammonium Chloride-screenshot (2).png'}]\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "data_dir=r\"./\" # Path to dataset (LabPics 1)\n",
    "data=[] # list of files in dataset\n",
    "for ff, name in enumerate(os.listdir(data_dir+\"dataset/Train/Image/\")):  # go over all folder annotation\n",
    "    data.append({\"image\":data_dir+\"dataset/Train/Image/\"+name,\"annotation\":data_dir+\"dataset/Train/Instance/\"+name[:-4]+\".png\"})\n",
    "print(data)\n",
    "def read_single(data): # read random image and single mask from  the dataset (LabPics)\n",
    "\n",
    "   #  select image\n",
    "\n",
    "        ent  = data[np.random.randint(len(data))] # choose random entry\n",
    "        Img = cv2.imread(ent[\"image\"])[...,::-1]  # read image\n",
    "        ann_map = cv2.imread(ent[\"annotation\"]) # read annotation\n",
    "\n",
    "   # resize image\n",
    "\n",
    "        r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor\n",
    "        Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "        ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n",
    "        if Img.shape[0]<1024:\n",
    "            Img = np.concatenate([Img,np.zeros([1024 - Img.shape[0], Img.shape[1],3],dtype=np.uint8)],axis=0)\n",
    "            ann_map = np.concatenate([ann_map, np.zeros([1024 - ann_map.shape[0], ann_map.shape[1],3], dtype=np.uint8)],axis=0)\n",
    "        if Img.shape[1]<1024:\n",
    "            Img = np.concatenate([Img, np.zeros([Img.shape[0] , 1024 - Img.shape[1], 3], dtype=np.uint8)],axis=1)\n",
    "            ann_map = np.concatenate([ann_map, np.zeros([ann_map.shape[0] , 1024 - ann_map.shape[1] , 3], dtype=np.uint8)],axis=1)\n",
    "\n",
    "   # merge vessels and materials annotations\n",
    "\n",
    "        mat_map = ann_map[:,:,0] # material annotation map\n",
    "        ves_map = ann_map[:,:,2] # vessel  annotaion map\n",
    "        mat_map[mat_map==0] = ves_map[mat_map==0]*(mat_map.max()+1) # merge maps\n",
    "\n",
    "   # Get binary masks and points\n",
    "\n",
    "        inds = np.unique(mat_map)[1:] # load all indices\n",
    "        if inds.__len__()>0:\n",
    "              ind = inds[np.random.randint(inds.__len__())]  # pick single segment\n",
    "        else:\n",
    "              return read_single(data)\n",
    "\n",
    "        #for ind in inds:\n",
    "        mask=(mat_map == ind).astype(np.uint8) # make binary mask corresponding to index ind\n",
    "        coords = np.argwhere(mask > 0) # get all coordinates in mask\n",
    "        yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n",
    "        return Img,mask,[[yx[1], yx[0]]]\n",
    "\n",
    "def read_batch(data,batch_size=4):\n",
    "      limage = []\n",
    "      lmask = []\n",
    "      linput_point = []\n",
    "      for i in range(batch_size):\n",
    "              image,mask,input_point = read_single(data)\n",
    "              limage.append(image)\n",
    "              lmask.append(mask)\n",
    "              linput_point.append(input_point)\n",
    "\n",
    "      return limage, np.array(lmask), np.array(linput_point),  np.ones([batch_size,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is used to load the SAM2 model and create a predictor instance.\n",
    "# It assumes that the SAM2 model files are located in the specified paths.\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cpu\")\n",
    "\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_21564\\650410587.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() # mixed precision\n"
     ]
    }
   ],
   "source": [
    "predictor.model.sam_mask_decoder.train(True) # enable training of mask decoder\n",
    "predictor.model.sam_prompt_encoder.train(True) # enable training of prompt encoder\n",
    "predictor.model.image_encoder.train(True) # enable training of image encoder: For this to work you need to scan the code for \"no_grad\" and remove them all\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)\n",
    "scaler = torch.cuda.amp.GradScaler() # mixed precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_21564\\1464669023.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(): # cast to mix precision\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-\u001b[32m1\u001b[39m])\u001b[38;5;66;03m# Upscale the masks to the original image resolution\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Segmentaion Loss caclulation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m gt_mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m prd_mask = torch.sigmoid(prd_masks[:, \u001b[32m0\u001b[39m])\u001b[38;5;66;03m# Turn logit map to probability map\u001b[39;00m\n\u001b[32m     24\u001b[39m seg_loss = (-gt_mask * torch.log(prd_mask + \u001b[32m0.00001\u001b[39m) - (\u001b[32m1\u001b[39m - gt_mask) * torch.log((\u001b[32m1\u001b[39m - prd_mask) + \u001b[32m0.00001\u001b[39m)).mean() \u001b[38;5;66;03m# cross entropy loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Btech\\Summer_Break\\SURGE\\Segment_Anything_Model_2\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:363\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    366\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "for itr in range(100000):\n",
    "    with torch.cuda.amp.autocast(): # cast to mix precision\n",
    "            image,mask,input_point, input_label = read_batch(data,batch_size=4) # load data batch\n",
    "            if mask.shape[0]==0: continue # ignore empty batches\n",
    "            predictor.set_image_batch(image) # apply SAM image encoder to the image\n",
    "            # predictor.get_image_embedding()\n",
    "            # prompt encoding\n",
    "\n",
    "            mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n",
    "            sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels),boxes=None,masks=None,)\n",
    "\n",
    "            # mask decoder\n",
    "\n",
    "            high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "            low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(image_embeddings=predictor._features[\"image_embed\"],image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),sparse_prompt_embeddings=sparse_embeddings,dense_prompt_embeddings=dense_embeddings,multimask_output=True,repeat_image=False,high_res_features=high_res_features,)\n",
    "            prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])# Upscale the masks to the original image resolution\n",
    "\n",
    "            # Segmentaion Loss caclulation\n",
    "\n",
    "            gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n",
    "            prd_mask = torch.sigmoid(prd_masks[:, 0])# Turn logit map to probability map\n",
    "            seg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean() # cross entropy loss\n",
    "\n",
    "            # Score loss calculation (intersection over union) IOU\n",
    "\n",
    "            inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "            iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "            score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "            loss=seg_loss+score_loss*0.05  # mix losses\n",
    "\n",
    "            # apply back propogation\n",
    "\n",
    "            predictor.model.zero_grad() # empty gradient\n",
    "            scaler.scale(loss).backward()  # Backpropogate\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update() # Mix precision\n",
    "\n",
    "            if itr%1000==0: torch.save(predictor.model.state_dict(), \"model.torch\") # save model\n",
    "\n",
    "            # Display results\n",
    "\n",
    "            if itr==0: mean_iou=0\n",
    "            mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "            print(\"step)\",itr, \"Accuracy(IOU)=\",mean_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
